{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beyond Grand Theft Auto V for Training, Testing, and Enhancing Deep Learning in Self Driving Cars\n",
    "\n",
    "## Training CNN to Estimate Affordance Variables on GTA V Screenshot data\n",
    "\n",
    "### General Information\n",
    "\n",
    "Code Author: Chawin Sitawarin (chawins@princeton.edu), Princeton University\n",
    "\n",
    "Please visit main website: https://princetonautonomous.github.io/ for a complete description of the project. It contains useful information as well as a link to the paper.\n",
    "\n",
    "- Format of the input images:  \n",
    "All images are .bmp by default and are all in a single directory. Dataset must be rescaled to the range [-0.9, 0.9] before feeding to the network. The code for scaling is provided [here](#Rescale).  \n",
    "\n",
    "- Format of the annotation file:  \n",
    "Annotation must be a plain text file with each line being:  \n",
    "`track_id, frame_id, angle, car_L, car_M, car_R, lane_LL, lane_L, lane_R, lane_RR\\n`  \n",
    "\n",
    "- Calculate pixel-wise mean of the training set before starting. We save it as one .bmp file with the same dimension as the images. Feel free to save it anyway you like, but the code will need to be very slightly modified.\n",
    "\n",
    "- Jupyter notebook does not have a functionality to keep on running if the browser closes. The file `run_alexnet.py` is a Python script that runs only the training portion but can be run with `nohup` command to keep it running even after the user logouts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import threading\n",
    "import time\n",
    "from os import listdir\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD\n",
    "from scipy import misc\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set path to directory containing images\n",
    "TRAIN_IMAGES = '/D/GTA_data/train/data/'\n",
    "# Set path to the annotation file\n",
    "TRAIN_ANNOT = '/D/GTA_data/train/annotation_scale.txt'\n",
    "\n",
    "TEST_IMAGES = '/D/GTA_data/test/data/'\n",
    "TEST_ANNOT = '/D/GTA_data/test/annotation_scale.txt'\n",
    "\n",
    "VALID_IMAGES = '/D/GTA_data/valid/data/'\n",
    "VALID_ANNOT = '/D/GTA_data/valid/annotation_scale.txt'\n",
    "\n",
    "# Set path to the mean of training set \n",
    "MEAN_IMAGE = '/D/GTA_data/train/mean.bmp'\n",
    "# Set Number of affordance variables\n",
    "NUM_LABELS = 8\n",
    "# Threshold on active/inactive state of affordance\n",
    "ACT_THRES = 0.99\n",
    "\n",
    "batch_size = 32                # Set size of the batch\n",
    "input_shape = (210, 280, 3)    # Set input shape of CNN\n",
    "output_dim = NUM_LABELS        # Number of output dimension     \n",
    "num_epoch = 100                # Number of epoch to train\n",
    "\n",
    "# Determine number of batches in the dataset\n",
    "train_files = [f for f in listdir(TRAIN_IMAGES)]\n",
    "num_steps = len(train_files) / batch_size\n",
    "\n",
    "test_files = [f for f in listdir(TEST_IMAGES)]\n",
    "test_num_steps = len(test_files) / batch_size\n",
    "\n",
    "valid_files = [f for f in listdir(VALID_IMAGES)]\n",
    "val_num_steps = len(valid_files) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup parameters for scaling and cleaning data\n",
    "UP_LIM = 100000\n",
    "angle_lim = 30      # Max limit for angle\n",
    "lane_dist_lim = 11  # Max limit for lane distance\n",
    "car_dist_lim = 60   # Max limit for car distance\n",
    "\n",
    "# Max values to scale [-0.9, 0.9]\n",
    "angle_max = float(angle_lim) * 10 / 9\n",
    "car_dist_max = float(car_dist_lim) * 10 / 9\n",
    "car_dist_inac = car_dist_max * 1.25\n",
    "\n",
    "lane_dist_max = lane_dist_lim * 10 / 9\n",
    "lane_dist_inac = lane_dist_max * 1.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model\n",
    "\n",
    "We use a modified version of AlexNet. There is an extra fully-connected layer at the end of the network with `output_dim` neurons and tanh as activation function. A new model can be easily built and replaces the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "# AlexNet with batch normalization in Keras \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(64, (11, 11), padding='same', input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "\n",
    "model.add(Convolution2D(128, (7, 7), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "\n",
    "model.add(Convolution2D(192, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "\n",
    "model.add(Convolution2D(256, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, kernel_initializer='normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(4096, kernel_initializer='normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1000, kernel_initializer='normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(output_dim, kernel_initializer='normal', activation='tanh'))\n",
    "\n",
    "adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer=adam, loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup a Generator\n",
    "\n",
    "A generator is needed to feed in samples to the network since dataset is too large to fit in the RAM. The code below is adapted from https://github.com/fchollet/keras/issues/1638."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class threadsafe_iter:\n",
    "    \"\"\"Takes an iterator/generator and makes it thread-safe by\n",
    "    serializing call to the `next` method of given iterator/generator.\n",
    "    \"\"\"\n",
    "    def __init__(self, it):\n",
    "        self.it = it\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def next(self):\n",
    "        with self.lock:\n",
    "            return self.it.next()\n",
    "\n",
    "\n",
    "def threadsafe_generator(f):\n",
    "    \"\"\"A decorator that takes a generator function and makes it thread-safe.\n",
    "    \"\"\"\n",
    "    def g(*a, **kw):\n",
    "        return threadsafe_iter(f(*a, **kw))\n",
    "    return g\n",
    "\n",
    "\n",
    "@threadsafe_generator\n",
    "def myGenerator(img_dir, ant_path, batch_size):  # write the definition of your data generator\n",
    "    \n",
    "    # List all files in directory\n",
    "    filenames = [f for f in listdir(img_dir)]\n",
    "    filenames.sort()\n",
    "    # Read annotation into a list\n",
    "    with open(ant_path) as ant:\n",
    "        annotation = ant.readlines()\n",
    "        annotation = [x.strip() for x in annotation] \n",
    "    \n",
    "    # Shuffle image filenames\n",
    "    random.seed(1234)\n",
    "    random.shuffle(filenames)\n",
    "    random.seed(1234)\n",
    "    random.shuffle(annotation)\n",
    "    \n",
    "    # Load mean image for mean subtraction\n",
    "    mean = cv2.imread(MEAN_IMAGE)\n",
    "    mean = cv2.cvtColor(mean, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    while 1:\n",
    "        for batch in range(len(filenames) / batch_size):\n",
    "            x_batch = np.zeros((batch_size, 210, 280, 3), dtype=np.float32)\n",
    "            y_batch = np.zeros((batch_size, output_dim), dtype=np.float32)\n",
    "            j = batch * batch_size\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                im = cv2.imread(img_dir + filenames[j + i])\n",
    "                im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "                x_batch[i] = im - mean\n",
    "                y_batch[i] = annotation[j + i].split(',')[2:]\n",
    "                \n",
    "            x_batch /= 255.\n",
    "            yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Callbacks\n",
    "\n",
    "Keras allows any number of customizable callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can use some built-in callbacks provided by Keras\n",
    "best_weights_filepath = 'weights.{epoch:02d}-{val_loss:.5f}.hdf5'\n",
    "earlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
    "saveBestModel = keras.callbacks.ModelCheckpoint(best_weights_filepath, monitor='val_loss', \n",
    "                                                verbose=1, save_best_only=True, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Alternatively, we can write our own. This one save all the weights every epoch.\n",
    "\n",
    "class SaveModel(keras.callbacks.Callback):\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    " \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    " \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.model.save_weights('./epoch' + str(epoch) + '.hdf5')\n",
    "        return\n",
    " \n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    " \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "saveModel = SaveModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load weights if you wish to resume training\n",
    "model.load_weights('./epoch10.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start training. Please refer to Keras official website for the function arguments\n",
    "model.fit_generator(myGenerator(TRAIN_IMAGES, TRAIN_ANNOT, batch_size), \n",
    "                    num_steps, epochs=num_epoch, verbose=1, \n",
    "                    validation_data=myGenerator(VALID_IMAGES, VALID_ANNOT, batch_size), \n",
    "                    validation_steps=val_num_steps, max_queue_size=100, \n",
    "                    workers=4, use_multiprocessing=False, initial_epoch=0,\n",
    "                    callbacks=[earlyStopping, saveBestModel])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.evaluate_generator(myGenerator(VALID_IMAGES, VALID_ANNOT, batch_size), \n",
    "                         val_num_steps, max_queue_size=100, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function that helps in evaluation\n",
    "\n",
    "def load_data(img_dir, ant_path):\n",
    "    \"\"\"Load a list of filenames in <img_dir> and annotation in <ant_path>\"\"\"\n",
    "    \n",
    "    # List all files in directory\n",
    "    filenames = [f for f in listdir(img_dir)]\n",
    "    filenames.sort()\n",
    "    # Read annotation into a list\n",
    "    with open(ant_path) as ant:\n",
    "        annotation = ant.readlines()\n",
    "        annotation = [x.strip() for x in annotation] \n",
    "\n",
    "    # Shuffle image filenames\n",
    "    random.seed(1234)\n",
    "    random.shuffle(filenames)\n",
    "    random.seed(1234)\n",
    "    random.shuffle(annotation)    \n",
    "\n",
    "    return filenames, annotation\n",
    "\n",
    "\n",
    "def predict(model, filepath):\n",
    "    \"\"\"Predict one image given <filepath>\"\"\"\n",
    "    \n",
    "    mean = cv2.imread(MEAN_IMAGE)\n",
    "    mean = cv2.cvtColor(mean, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    im = cv2.imread(filepath)\n",
    "    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "    im = (im - mean) / 255.\n",
    "    im = im.reshape((1, 210, 280, 3))\n",
    "    \n",
    "    return model.predict(im, batch_size=1, verbose=0)[0]\n",
    "\n",
    "\n",
    "def invert_scale(y):\n",
    "    \"\"\"Invert scaling back to original values\"\"\"\n",
    "    \n",
    "    y_invert = np.zeros((8, ))\n",
    "    y_invert[0] = y[0] * angle_max\n",
    "    y_invert[1] = (y[1] * car_dist_max + car_dist_lim) / 2\n",
    "    y_invert[2] = (y[2] * car_dist_max + car_dist_lim) / 2\n",
    "    y_invert[3] = (y[3] * car_dist_max + car_dist_lim) / 2\n",
    "    y_invert[4] = (y[4] * lane_dist_max + lane_dist_lim) / 2\n",
    "    y_invert[5] = (y[5] * lane_dist_max + lane_dist_lim) / 2\n",
    "    y_invert[6] = (y[6] * lane_dist_max + lane_dist_lim) / 2\n",
    "    y_invert[7] = (y[7] * lane_dist_max + lane_dist_lim) / 2\n",
    "    \n",
    "    return y_invert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Show estimated labels vs ground truth as well as show the image\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    index = random.choice(range(len(filenames)))\n",
    "    print index\n",
    "    filepath = VALID_IMAGES + filenames[index]\n",
    "    \n",
    "    y_pred = predict(model, filepath)\n",
    "    y_true = annot[index]\n",
    "    \n",
    "    print invert_scale(predict(model, filepath))\n",
    "    print invert_scale(annot[index])\n",
    "\n",
    "    im = misc.imread(filepath)\n",
    "    plt.imshow(im)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate error for each variable separately.\n",
    "# Also exclude error coming from inactive affordance\n",
    "\n",
    "filenames, annotation = load_data(VALID_IMAGES, VALID_ANNOT)\n",
    "\n",
    "annot = np.zeros((len(annotation), NUM_LABELS))\n",
    "for i in range(len(annotation)):\n",
    "    annot[i] = annotation[i].split(',')[2:] \n",
    "\n",
    "error = np.zeros(8, )\n",
    "\n",
    "for index in range(len(filenames)):\n",
    "\n",
    "    filepath = VALID_IMAGES + filenames[index]\n",
    "    \n",
    "    y_pred = predict(model, filepath)\n",
    "    y_true = annot[index]\n",
    "    \n",
    "    for j in range(NUM_LABELS):\n",
    "        \n",
    "        if j == 0:\n",
    "            error[j] += (y_pred[j] - y_true[j]) ** 2\n",
    "        else:\n",
    "            dist_pred = y_pred[j] if y_pred[j] < ACT_THRES else 1\n",
    "            dist_true = y_true[j] if y_true[j] < ACT_THRES else 1\n",
    "            error[j] += (dist_pred - dist_true) ** 2\n",
    "            \n",
    "print error / len(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Up Data\n",
    "\n",
    "This section of code cleans up bad data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMAGES_RAW = '/D/GTA_data/valid/raw/'\n",
    "IMAGES_CLEAN = '/D/GTA_data/valid/data/'\n",
    "\n",
    "ANNOT_RAW = '/D/GTA_data/valid/annotation_raw.txt'\n",
    "ANNOT_CLEAN = '/D/GTA_data/valid/temp.txt'\n",
    "ANNOT_SCALE = '/D/GTA_data/valid/annotation_scale.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from shutil import copyfile\n",
    "\n",
    "# Rename files to zero-leading\n",
    "filenames = [f for f in listdir(IMAGES_RAW)]\n",
    "for fn in filenames:\n",
    "    track_nb = fn.split('_')[0].zfill(5)\n",
    "    frame_nb = fn.split('_')[1].split('.')[0].zfill(4)\n",
    "    os.rename(IMAGES_RAW + fn, IMAGES_RAW + track_nb + '_' + frame_nb + '.bmp')\n",
    "    \n",
    "# Read annotation to list\n",
    "with open(ANNOT_RAW) as ant:\n",
    "    annotation = ant.readlines()\n",
    "    annotation = [x.strip() for x in annotation] \n",
    "\n",
    "annot = np.zeros((len(annotation), NUM_LABELS))\n",
    "for i in range(len(annotation)):\n",
    "    annot[i] = annotation[i].split(',')[2:]   \n",
    "\n",
    "del_id = []\n",
    "frames_with_car = np.zeros((3, ), dtype=np.int32)\n",
    "\n",
    "# Iterate through data to find indices of bad data\n",
    "for i in range(len(annotation)):\n",
    "    for j in range(NUM_LABELS):\n",
    "        \n",
    "        # Clean up too large angle\n",
    "        if j == 0:\n",
    "            if annot[i, j] < 0:\n",
    "                if annot[i, j] < -angle_lim and annot[i, j] > -360 + angle_lim:\n",
    "                    del_id.append(i)\n",
    "                    break\n",
    "            else:\n",
    "                if annot[i, j] > angle_lim and annot[i, j] < 360 - angle_lim:\n",
    "                    del_id.append(i)\n",
    "                    break\n",
    "        \n",
    "        # Clean up bad car dist\n",
    "        if j >= 1 and j <= 3:\n",
    "            if annot[i, j] < UP_LIM:\n",
    "                frames_with_car[j - 1] += 1\n",
    "                if annot[i, j] > car_dist_lim:\n",
    "                    del_id.append(i)\n",
    "                    break\n",
    "            else:\n",
    "                if (frames_with_car[j - 1] > 0) and (frames_with_car[j - 1] <= 5):\n",
    "                    for k in range(frames_with_car[j - 1]):\n",
    "                        del_id.append(i - k - 1)\n",
    "                frames_with_car[j - 1] = 0\n",
    "        \n",
    "        # Clean up lane distance\n",
    "        if j >= 4 and j <= 7:\n",
    "            if (annot[i, j] > lane_dist_lim and annot[i, j] < UP_LIM) or (annot[i, j] < 0):\n",
    "                del_id.append(i)\n",
    "                break\n",
    "\n",
    "del_id = list(set(del_id))\n",
    "print 'Number of bad data: ', len(del_id)\n",
    "print 'Number of clean data: ', (len(annotation) - len(del_id))\n",
    "\n",
    "# Remove bad data\n",
    "mask = np.ones(len(annotation), dtype=bool)\n",
    "mask[del_id] = False\n",
    "filenames = [f for f in listdir(IMAGES_RAW)]\n",
    "filenames.sort()\n",
    "cleaned_annotation = np.array(annotation)[mask]\n",
    "cleaned_filenames = np.array(filenames)[mask]\n",
    "\n",
    "# Copy cleaned data to new directory\n",
    "for fn in cleaned_filenames:\n",
    "    copyfile(IMAGES_RAW + fn, IMAGES_CLEAN + fn)\n",
    "# Write cleaned annotation to new file\n",
    "f = open(ANNOT_CLEAN, 'a')\n",
    "for ant in cleaned_annotation:\n",
    "    f.write(ant + '\\n') \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescale\n",
    "\n",
    "Rescale all labels to range [-1, 1] ([-0.9, 0.9] in practice to leave a slight margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read annotation to list\n",
    "with open(ANNOT_CLEAN) as ant:\n",
    "    annotation = ant.readlines()\n",
    "    annotation = [x.strip() for x in annotation] \n",
    "\n",
    "annot = np.zeros((len(annotation), NUM_LABELS))\n",
    "for i in range(len(annotation)):\n",
    "    annot[i] = annotation[i].split(',')[2:] \n",
    "    \n",
    "max_annot = np.zeros((NUM_LABELS, ))\n",
    "for i in range(len(annotation)):\n",
    "    for j in range(NUM_LABELS):\n",
    "        if annot[i, j] <= UP_LIM and annot[i, j] > max_annot[j]:\n",
    "            max_annot[j] = annot[i, j]\n",
    "\n",
    "min_annot = np.min(annot, axis=0)\n",
    "\n",
    "print 'Max: ', max_annot\n",
    "print 'Min: ', min_annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(ANNOT_SCALE, 'a')\n",
    "\n",
    "for i in range(len(annotation)):\n",
    "    \n",
    "    out = annotation[i].split(',')[0].zfill(5)\n",
    "    out += ', ' + annotation[i].split(',')[1].strip().zfill(4)\n",
    "    \n",
    "    if annot[i, 0] > 360 - angle_lim:\n",
    "        angle = annot[i, 0] - 360\n",
    "    elif annot[i, 0] < -360 + angle_lim:\n",
    "        angle = annot[i, 0] + 360 \n",
    "    else:\n",
    "        angle = annot[i, 0]\n",
    "    out += ', ' + `angle / angle_max`\n",
    "    \n",
    "    # Rescale car dist from [0, 60] to [-0.9, 0.9]\n",
    "    for j in range(1, 4):\n",
    "        if annot[i, j] > UP_LIM:\n",
    "            dist = car_dist_inac\n",
    "        else:\n",
    "            dist = annot[i, j]\n",
    "        out += ', ' + `(2 * dist - car_dist_lim) / car_dist_max`\n",
    "    \n",
    "    # Rescale lane dist from [0, 11] to [-0.9, 0.9]\n",
    "    for j in range(4, 8):\n",
    "        if annot[i, j] > UP_LIM:\n",
    "            dist = lane_dist_inac\n",
    "        else:\n",
    "            dist = annot[i, j]\n",
    "        out += ', ' + `(2 * dist - lane_dist_lim) / lane_dist_max`\n",
    "    \n",
    "    f.write(out + '\\n')\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read annotation to list\n",
    "with open(ANNOT_SCALE) as ant:\n",
    "    annotation = ant.readlines()\n",
    "    annotation = [x.strip() for x in annotation] \n",
    "\n",
    "annot = np.zeros((len(annotation), NUM_LABELS))\n",
    "for i in range(len(annotation)):\n",
    "    annot[i] = annotation[i].split(',')[2:] \n",
    "    \n",
    "max_annot = np.zeros((NUM_LABELS, ))\n",
    "for i in range(len(annotation)):\n",
    "    for j in range(NUM_LABELS):\n",
    "        if annot[i, j] <= UP_LIM and annot[i, j] > max_annot[j]:\n",
    "            max_annot[j] = annot[i, j]\n",
    "\n",
    "min_annot = np.min(annot, axis=0)\n",
    "\n",
    "print 'Max: ', max_annot\n",
    "print 'Min: ', min_annot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
